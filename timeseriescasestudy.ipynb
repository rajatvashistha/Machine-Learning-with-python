{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"\" width=\"240\" height=\"360\" />\n",
    "\n",
    "# TIME SERIES ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/aooy8xP.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e025f18744ae4c8e784aec0d1ea92a48dde55995"
   },
   "source": [
    "## Table of contents\n",
    "1. [Objective](#Objective)\n",
    "2. [Data Loading and Visualization](#Data Loading and Visualization)\n",
    "3. [Decomposition](#Decomposition)\n",
    "4. [Stationarity Check](#Stationarity Check)<br/>\n",
    "    4.1. [Rolling Statistics Methodology](#Rolling Statistics Methodology)<br/>\n",
    "    4.2. [ADF(Augmented Dickey-Fuller)Test](#ADF)   \n",
    "5. [Data Transformation to achieve Stationarity](#Data Transformation to achieve Stationarity)<br/>\n",
    "    5.1. [Log Scale Transformation](#Log Scale Transformation)<br/>\n",
    "    5.2. [Log Scale - Moving Average Transformation](#Log Scale - Moving Average Transformation)<br/>\n",
    "    5.3. [Exponential Decay Transformation](#Exponential Decay Transformation)<br/>\n",
    "    5.4. [Time Shift Transformation](#Time Shift Transformation)<br/>\n",
    "6. [Plotting ACF & PACF](#Plotting ACF & PACF)\n",
    "7. [Building Models](#Building Models)<br/>\n",
    "    7.1. [AR Model](#AR Model)<br/>\n",
    "    7.2. [ARMA Model](#ARMA Model)<br/>\n",
    "    7.3. [ARIMA Model](#ARIMA Model)<br/>\n",
    "8. [Extra Material ](#Extra Material )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "36fd2490ab780dd1298aae2c14631950aefab8ef"
   },
   "source": [
    "### 1. Objective <a name = \"Objective\"></a>\n",
    "- Build a model to forecast the demand ( passenger traffic) in Airplanes.\n",
    "- The data is classified in date/time and the passengers travelling per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Visualization <a name = \"Data Loading and Visualization\"></a>\n",
    "\n",
    "__Import the dataset__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 11001] getaddrinfo failed>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[1;32m-> 1317\u001b[1;33m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[0;32m   1318\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1228\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1229\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1274\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1275\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1223\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1224\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1015\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    955\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 956\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    957\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    927\u001b[0m         self.sock = self._create_connection(\n\u001b[1;32m--> 928\u001b[1;33m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[0;32m    929\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3e03ecdb5474>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m  \u001b[1;33m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://raw.githubusercontent.com/insaid2018/Term-3/master/Data/CaseStudy/AirPassengers.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_infer_compression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(\n\u001b[1;32m--> 413\u001b[1;33m         filepath_or_buffer, encoding, compression)\n\u001b[0m\u001b[0;32m    414\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'compression'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_urlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Content-Encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'gzip'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 543\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1358\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[1;32m-> 1360\u001b[1;33m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1317\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0;32m   1318\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1319\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1320\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [Errno 11001] getaddrinfo failed>"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "dataset  =  pd.read_csv('https://raw.githubusercontent.com/insaid2018/Term-3/master/Data/CaseStudy/AirPassengers.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f696360018577b8163554b95454e24edb04cdb43"
   },
   "source": [
    "Time series deals with 2 columns,\n",
    "- one is temporal i.e: __month__ in this case &\n",
    "- another is the value to be forecasted ie: __airplane passengers__. <br/>\n",
    "\n",
    "\n",
    "To make plotting graphs easier, we set __Month as the index__ of pandas dataframe as during plots, the index will act by default as the x-axis & since it has only 1 more column, that will be automatically taken as the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1d2a64e9b0d8b8485936e973468b2ccb25e706c2"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dataset['Month']  =  pd.to_datetime(dataset['Month'],infer_datetime_format = True)             #convert from string to datetime\n",
    "\n",
    "indexedDataset  =  dataset.set_index(['Month'])\n",
    "indexedDataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.randn(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.empty([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.empty([2,2],dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualizing the dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/UgzDMlj.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "761cd88f2629f435defbe769b7bb0b0e320c97d1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline                        \n",
    "\n",
    "plt.xlabel('Yearl')\n",
    "plt.ylabel('Number of air passengers')\n",
    "plt.plot(indexedDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e71de03beca98002540379de58943270006a6ae"
   },
   "source": [
    "From the plot below, we can see that there is a __Trend compoenent__ in the series.<br/> Now for better clarity lets decompose the time series in its constituent components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Decomposition <a name = \"Decomposition\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize']  =  10, 5\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposed_dataset  =  seasonal_decompose(indexedDataset)         \n",
    "figure  =  decomposed_dataset.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stationarity Check <a name = \"Stationarity Check\"></a>\n",
    "\n",
    "We will see two methods two check stationarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/TOsgav1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a function to check stationarity in one go using both __rolling statistics plot and ADF test__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(time_series):\n",
    "    rolling_means(time_series)\n",
    "    adf_test(time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Rolling Statistics Methodology <a name = \"Rolling Statistics Methodology\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "720f4fff3d03fc7daa98d4412f0bcddf1f2d89fa"
   },
   "outputs": [],
   "source": [
    "# Determine rolling statistics\n",
    "def rolling_means(time_series):\n",
    "    rolmean  =  time_series.rolling(window = 12).mean()    #window size 12 denotes 12 months, giving rolling mean at yearly level\n",
    "    rolstd  =  time_series.rolling(window = 12).std()\n",
    "\n",
    "    #Plot rolling statistics\n",
    "\n",
    "    rcParams['figure.figsize']  =  10, 5 \n",
    "\n",
    "    orig  =  plt.plot(time_series, color = 'blue', label = 'Original')\n",
    "    mean  =  plt.plot(rolmean, color = 'red', label = 'Rolling Mean')\n",
    "    std  =  plt.plot(rolstd, color = 'black', label = 'Rolling Std')\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_means(indexedDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9dd9317997eaf9d86c1615b144d832d9a2547cb9"
   },
   "source": [
    "- Rolling mean has a __trend__ component \n",
    "- Rolling standard deviation is fairly __constant__ with time.\n",
    "\n",
    "For our time series to be stationary, we need to ensure that both the __rolling statistics__ i.e: __mean & stdandard deviation remain time invariant__ or constant with time. Thus the curves for both of them have to be parallel to the x-axis, which in our case is not so. \n",
    "\n",
    "To further augment our hypothesis that the time series is not stationary, let us perform the __ADCF test__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2  ADF(Augmented Dickey-Fuller) Test <a name = \"ADF\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe828fcb190363b6aba127ecea7f5b7e1fd554bc"
   },
   "source": [
    "For a Time series to be __stationary__, its ADF test should have:\n",
    "1. __low p-value__ (according to the null hypothesis)\n",
    "2. __Critical values__ at 1%, 5%, 10% confidence intervals should be as __close__ as possible __to__ the __Test Statistics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "424d7f23806906f44e793442fdd451987c9b33d7"
   },
   "outputs": [],
   "source": [
    "#Perform Augmented Dickeyâ€“Fuller test:\n",
    "def adf_test(time_series):\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    dftest  =  adfuller(time_series['#Passengers'], autolag = 'AIC')\n",
    "\n",
    "    dfoutput  =  pd.Series(dftest[0:4], index = ['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key]  =  value\n",
    "    \n",
    "    print('Results of Dickey Fuller Test:')    \n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(indexedDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe828fcb190363b6aba127ecea7f5b7e1fd554bc"
   },
   "source": [
    "- __large p-value__.\n",
    "- Also critical values (1%, 5%, 10%) are __no where close to__ the Test Statistics.\n",
    "\n",
    "Hence, we can safely say that **our Time Series at the moment is not stationary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff2e442d4169e7c979efbd402e835c01c0b2c6fc"
   },
   "source": [
    "### 5. Data Transformation to achieve Stationarity <a name = \"Data Transformation to achieve Stationarity\"></a>\n",
    "\n",
    "There are a couple of ways to achieve stationarity through data transformation like taking $log_{10}$,$log_{e}$, square, square root, cube, cube root, exponential decay, time shift and so on ...\n",
    "\n",
    "In our notebook, lets start of with log transformations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2daca62f26cb0736d99a037b8cfac78f4e2c3178"
   },
   "source": [
    "#### 5.1 Log Scale Transformation  <a name = \"Log Scale Transformation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90510fb0b230444f44eee765824a7c6dd2fb4b5a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "indexedDataset_logScale  =  np.log(indexedDataset)\n",
    "rcParams['figure.figsize']  =  10, 5\n",
    "plt.plot(indexedDataset_logScale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3022a9239558aca693cf4607f00a9eaa1cfce37b"
   },
   "outputs": [],
   "source": [
    "test_stationarity(indexedDataset_logScale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ac9dc414702d89049c3ad226e4f8fc7d6514dee"
   },
   "source": [
    "From above graph, we see that even though rolling mean is __not stationary__, it is still better than the previous case, where no transfromation were applied to series. So we can atleast say that we are heading in the right direction.\n",
    "\n",
    "We know from above graph that both the Time series with log scale as well as its moving average have a trend component. Thus we can apply a elementary intuition: subtraction one from the other should remove the trend component of both. Its like:  \n",
    "\n",
    "$log scale L  =  stationary part(L1) + trend(LT)$   \n",
    "$moving avg of log scale A  =  stationary part(A1) + trend(AT)$   \n",
    "$result series R  =  L - A  =  (L1+LT) - (A1+AT)  =  (L1-A1) + (LT-AT)$\n",
    "\n",
    "Since, L & A are series & it moving avg, their trend will be more or less same, Hence  \n",
    "LT-AT nearly equals to 0  \n",
    "\n",
    "Thus trend component will be almost removed. And we have,  \n",
    "  \n",
    "$R  =  L1 - A1$, our final non-trend curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Log Scale - Moving Average Transformation  <a name = \"Log Scale - Moving Average Transformation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b022e936f5b22d9e1847190abe37c68012fd3b95"
   },
   "outputs": [],
   "source": [
    "movingAverage  =  indexedDataset_logScale.rolling(window = 12).mean()\n",
    "datasetLogScaleMinusMovingAverage  =  indexedDataset_logScale - movingAverage\n",
    "plt.plot(datasetLogScaleMinusMovingAverage)\n",
    "\n",
    "#Remove NAN values\n",
    "datasetLogScaleMinusMovingAverage.dropna(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3022a9239558aca693cf4607f00a9eaa1cfce37b"
   },
   "outputs": [],
   "source": [
    "test_stationarity(datasetLogScaleMinusMovingAverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8df1ecf50a8e19504cdd513930364fb23307baa"
   },
   "source": [
    "- p-value has __reduced__ from 0.99 to 0.022.  \n",
    "- The __critical values__ at 1%,5%,10% confidence intervals are pretty __close to the Test Statistic__.\n",
    "\n",
    "Thus, from above 2 points, we can say that our given series is stationary.  \n",
    "\n",
    "From above graph, we observe that our intuition that *\"subtracting two related series having similar trend components will make the result stationary\"* is true.   \n",
    "\n",
    "\n",
    "\n",
    "But, in the spirit of getting higher accuracy, let us explore & try to find a better scale than our current log.\n",
    "\n",
    "Let us try out Exponential decay.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35a7bdf0fa22bf5776a4d5a715e0531b2688a889"
   },
   "source": [
    "### 5.3 Exponential Decay Transformation   <a name = \"Exponential Decay Transformation \"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0a6d6f496870b20abaf04d889496d6db921759e"
   },
   "outputs": [],
   "source": [
    "exponentialDecayWeightedAverage  =  indexedDataset_logScale.ewm(halflife = 12, min_periods = 0, adjust = True).mean()\n",
    "plt.plot(indexedDataset_logScale)\n",
    "plt.plot(exponentialDecayWeightedAverage, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3022a9239558aca693cf4607f00a9eaa1cfce37b"
   },
   "outputs": [],
   "source": [
    "test_stationarity(exponentialDecayWeightedAverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4443cd76dc7af382438df8d0ab10217560891ff8"
   },
   "source": [
    "From above graph, it seems that exponential decay is not holding any advantage over log scale as both the corresponding curves are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2e6f7b573fd615b18fbcd3c8ea5ef1e0549d28e"
   },
   "outputs": [],
   "source": [
    "datasetLogScaleMinusExponentialMovingAverage  =  indexedDataset_logScale - exponentialDecayWeightedAverage\n",
    "test_stationarity(datasetLogScaleMinusExponentialMovingAverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14d7b74db220e1f9ae575d8eaa5cbd4b78e996ed"
   },
   "source": [
    "We observe that the Time Series is stationary & also the series for moving avg & std. dev. is almost parallel to x-axis thus they also have no trend.  \n",
    "Also,     \n",
    "1. p-value has __decreased__ from 0.022 to 0.005.  \n",
    "2. Test Statistic value is very __much closer__ to the Critical values.  \n",
    "\n",
    "\n",
    "Both the points say that our current transformation is better than the previous logarithmic transformation. Even though, we couldn't observe any differences by visually looking at the graphs, the tests confirmed decay to be much better.\n",
    "\n",
    "But lets try one more time & find if an even better solution exists. We will try out the simple time shift technique, which is simply:  \n",
    "\n",
    "Given a set of observation on the time series:  \n",
    "$ x0, x1, x2, x3, .... xn $  \n",
    "\n",
    "The shifted values will be:    \n",
    "$ null, x0, x1, x2,.... xn $                             <---- basically all xi's shifted by 1 pos to right  \n",
    "\n",
    "Thus, the time series with time shifted values are:   \n",
    "$ null, (x1-x0), (x2-x1), (x3-x2), (x4-x3),.... (xn-x_{n-1}) $   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62d6495976ac9a3e825fc033982db13b60635150"
   },
   "source": [
    "### 5.4 Time Shift Transformation  <a name = \"Time Shift Transformation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c6ebf0391698cf9dc74b7f550c2e9b6eb74d17d9"
   },
   "outputs": [],
   "source": [
    "datasetLogDiffShifting  =  indexedDataset_logScale - indexedDataset_logScale.shift()\n",
    "plt.plot(datasetLogDiffShifting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4670612db47e5c43b47405e2e56129a95e055a16"
   },
   "outputs": [],
   "source": [
    "datasetLogDiffShifting.dropna(inplace = True)\n",
    "\n",
    "test_stationarity(datasetLogDiffShifting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df10f819ee7cd6475a527db13acbbdb6958d2e1f"
   },
   "source": [
    "From above 2 graphs, we can see that, visually this is the best result as our series along with rolling statistic values of moving avg & moving std. dev. is very much flat & stationary. But, the ADCF test shows us that:\n",
    "1. p-value of 0.07 is not as good as 0.005 of exponential decay.  \n",
    "2. Test Statistic value not as close to the critical values as that for exponential decay.  \n",
    "  \n",
    "We have thus tried out 3 different transformation: log, exp decay & time shift. We will go ahead with the __timeshifted dataset.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "02256626044f733610cfe6920509385abc89dadd"
   },
   "source": [
    "### 6. Plotting ACF & PACF <a name = \"Plotting ACF & PACF\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f7e2bd9d15b36aa5bfbd41a3ea5b99ef9d2124e"
   },
   "outputs": [],
   "source": [
    "#ACF & PACF plots\n",
    "\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "lag_acf  =  acf(datasetLogDiffShifting, nlags = 20)\n",
    "lag_pacf  =  pacf(datasetLogDiffShifting, nlags = 20, method = 'ols')\n",
    "rcParams['figure.figsize']  =  10, 5\n",
    "#Plot ACF:\n",
    "plt.subplot(121)\n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y = 0, linestyle = '--', color = 'gray')\n",
    "plt.axhline(y = -1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle = '--', color = 'gray')\n",
    "plt.axhline(y = 1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle = '--', color = 'gray')\n",
    "plt.xticks(np.arange(0,22,2))\n",
    "plt.title('Autocorrelation Function')            \n",
    "\n",
    "#Plot PACF\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y = 0, linestyle = '--', color = 'gray')\n",
    "plt.axhline(y = -1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle = '--', color = 'gray')\n",
    "plt.axhline(y = 1.96/np.sqrt(len(datasetLogDiffShifting)), linestyle = '--', color = 'gray')\n",
    "plt.xticks(np.arange(0,22,2))\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "            \n",
    "plt.tight_layout()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccbd3aa17eb33306ec7f17fa54bfe2bbcad98f11"
   },
   "source": [
    "From the __ACF graph__, \n",
    "- Curve touches y = 0.0 line at x = 2. Thus, __Q  =  2__\n",
    "\n",
    "\n",
    "From the __PACF graph__,\n",
    "- Curve touches y = 0.0 line at x = 2. Thus, __P  =  2__\n",
    "\n",
    "ARIMA is AR + I + MA. Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower RSS values indicate a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f8a4b371f1678d3fd84b715c433438f74b91e1f"
   },
   "source": [
    "### 7. Building Models <a name = \"Building Models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccbd3aa17eb33306ec7f17fa54bfe2bbcad98f11"
   },
   "source": [
    "Let us forecaste deploying AR, ARMA and ARIMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 AR Model <a name = \"AR Model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using AR model to forecast the number of passengers for __7 months__. \n",
    "- Dividing the dataset into __train and test__. Keeping last 7 months data for testing the performance of our time series model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetLogDiffShifting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "train, test = datasetLogDiffShifting[1:len(datasetLogDiffShifting)-7], datasetLogDiffShifting[len(datasetLogDiffShifting)-7:]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deploying the autoregression model of __statsmodels library__ provided in the ar_model class.\n",
    "- It automatically selects an appropriate lag value using statistical tests and trains a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AR\n",
    "model = AR(train)\n",
    "model_fit = model.fit()\n",
    "print('Lag: %s' % model_fit.k_ar)\n",
    "print('Coefficients: %s' % model_fit.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe that a __13-lag model__ was chosen and trained. This is interesting given how close this lag is to the number of months in a year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Forecasting__ using the developed model and printing out the __7 month forecast__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "predictions.head()\n",
    "for i in range(len(predictions)):\n",
    "    print('predicted=%f, expected=%f' % (predictions[i], test.iloc[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Model evaluation__ using Mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)\n",
    "# plot results\n",
    "\n",
    "pyplot.plot(test)\n",
    "pyplot.plot(predictions, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the plot of <span style=\"color:blue\">**__expected__** </span> vs the <span style=\"color:red\">**__predicted__** </span>.\n",
    "\n",
    "The forecast does look pretty good with slightly large deviation on month 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train)\n",
    "plt.plot(model_fit.fittedvalues, color = 'red')\n",
    "print('Plotting AR model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 ARMA Model <a name = \"ARMA Model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA\n",
    "model = ARMA(train,order=(2,2))\n",
    "model_fit = model.fit()\n",
    "print('Lag: %s' % model_fit.k_ar)\n",
    "print('Coefficients: %s' % model_fit.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe that a __2-lag model__ was chosen and trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Forecasting__ using the developed model and printing out the __7 month forecast__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "for i in range(len(predictions)):\n",
    "    print('predicted=%f, expected=%f' % (predictions[i], test.iloc[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Model evaluation__ using Mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)\n",
    "# plot results\n",
    "\n",
    "pyplot.plot(test)\n",
    "pyplot.plot(predictions, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE is more than that obtained with AR model. <br/>\n",
    "Observing the plot of <span style=\"color:blue\">**__expected__** </span> vs the <span style=\"color:red\">**__predicted__** </span>.\n",
    "The forecast doesnt look good with large deviation at 3 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train)\n",
    "plt.plot(model_fit.fittedvalues, color = 'red')\n",
    "print('Plotting ARMA model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alternatively you can also use __plot_predict()__ method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = model_fit.plot_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 ARIMA Model <a name = \"ARIMA Model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training ARIMA model with __stationary__ made dataset \"datasetLogDiffShifting\" using the (p,d,q) = __(2,0,2)__. \n",
    "- p and q values are chosen considering observations of ACF and PACF plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "model = ARIMA(datasetLogDiffShifting,order=(2,0,2))\n",
    "print('Lag: %s' % model_fit.k_ar)\n",
    "print('Coefficients: %s' % model_fit.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe that a __2-lag model__ was chosen and trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Model evaluation__ using Mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ARIMA = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "error = mean_squared_error(datasetLogDiffShifting, results_ARIMA.fittedvalues)\n",
    "print('Test MSE: %.3f' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ARIMA.fittedvalues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetLogDiffShifting.plot()\n",
    "results_ARIMA.fittedvalues.plot(color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alternatively you can also use __plot_predict()__ method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_values = results_ARIMA.plot_predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9aad48b39d784b198a2ceb64f6525d8bf010df78"
   },
   "source": [
    "### Prediction & Reverse transformations of fittedvalues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\n",
    "predictions_ARIMA_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "predictions_ARIMA_diff_cumsum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_log = pd.Series(indexedDataset_logScale['#Passengers'].iloc[0], index=datasetLogDiffShifting.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\n",
    "predictions_ARIMA_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverted_back_prediction = pd.DataFrame(np.exp(predictions_ARIMA_log))\n",
    "reverted_back_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(indexedDataset)\n",
    "pyplot.plot(reverted_back_prediction, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a0bcbd64aef08232010e737293b88405d4d5f662"
   },
   "source": [
    "We see that our predicted forecasts are very close to the real time series values indicating a fairly accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexedDataset_logScale.columns = ['Passengers in logscale']\n",
    "reverted_back_prediction.columns = ['ARIMA Predicted passengers']\n",
    "datasetLogDiffShifting.columns = ['Passengers after log diff shifting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([indexedDataset,indexedDataset_logScale, datasetLogDiffShifting,reverted_back_prediction], axis=1, sort=False)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting the number of air passengers for the next 10 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have 144(existing data of 12 yrs in months) data points. <br/>\n",
    "To forecast additional 120 data points or __10 yrs__.\n",
    "- Using plot_predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca204ccf1ab4ba28a83dcffdb50d6d300123fc20"
   },
   "outputs": [],
   "source": [
    "results_ARIMA.plot_predict(1,264) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_yr_forecast = results_ARIMA.predict(start=len(datasetLogDiffShifting),end = len(datasetLogDiffShifting) + 120 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9aad48b39d784b198a2ceb64f6525d8bf010df78"
   },
   "source": [
    "### Prediction & Reverse transformations of 10 year forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff = pd.Series(ten_yr_forecast, copy=True)\n",
    "predictions_ARIMA_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "predictions_ARIMA_diff_cumsum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_log = pd.Series(indexedDataset_logScale['Passengers in logscale'].iloc[-1], index=predictions_ARIMA_diff.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\n",
    "predictions_ARIMA_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverted_back_prediction = pd.DataFrame(np.exp(predictions_ARIMA_log))\n",
    "reverted_back_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(indexedDataset)\n",
    "pyplot.plot(reverted_back_prediction, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Extra Material  <a name = \"Extra Material\"></a>\n",
    "\n",
    "Using ARIMA for making AR and MA models by setting the values of (p,d,q) as (2,1,0) and (0,1,2) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1fcf782c93eb35a746d0a107aa44d41f62290e4"
   },
   "outputs": [],
   "source": [
    "#AR Model\n",
    "#making order = (2,1,0) gives RSS = 1.5023\n",
    " \n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "model  =  ARIMA(indexedDataset_logScale, order = (2,1,0))\n",
    "results_AR  =  model.fit()\n",
    "plt.plot(datasetLogDiffShifting)\n",
    "plt.plot(results_AR.fittedvalues, color = 'red')\n",
    "plt.title('RSS: %.4f'%sum((results_AR.fittedvalues - datasetLogDiffShifting['Passengers after log diff shifting'])**2))\n",
    "print('Plotting AR model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef6f3af5945c7ba92eda2c0cba9dd4b5b3e6b02a"
   },
   "outputs": [],
   "source": [
    "#MA Model\n",
    "model  =  ARIMA(indexedDataset_logScale, order = (0,1,2))\n",
    "results_MA  =  model.fit()\n",
    "plt.plot(datasetLogDiffShifting)\n",
    "plt.plot(results_MA.fittedvalues, color = 'red')\n",
    "plt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting['Passengers after log diff shifting'])**2))\n",
    "print('Plotting MA model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that, these models will give a value of RSS. __Lower RSS values indicate a better model.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fad66003073e3ca600d1c159b8c0968c50f4377f"
   },
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
